{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4e1a48e",
   "metadata": {},
   "source": [
    "Empezar a ver lo de query expansion con clinical notes. \n",
    "* Procesar los datos.\n",
    "* Replicar el análisis del paper.\n",
    "    * Incluir la búsqueda con las keywords de expertos y combinar eso con las alternativas\n",
    "* Agregar alternativas de ranking, como por ejemplo, alguna semántica.\n",
    "\n",
    "[Investigating the Impact of Query Representation on Medical Information Retrieval](https://drive.google.com/file/d/1V9P9idStG4Gfaf4ZZ5Gpn6ldIk0Zcdq_/view)\n",
    "\n",
    "Tienen repo: https://github.com/GiorgosPeikos/inf_extraction_med_ir\n",
    "\n",
    "Mucho código, dudo que todo sea absolutamente necesario. De base usan pyterrier. La creación del index no está, pero si la del retrieval y también la evaluación (usan las métricas del paquete).\n",
    "\n",
    "Ver en qué formato hay que darle los datos de los qrels (creo que son esos para que haga la evaluación solo)\n",
    "\n",
    "https://www.trec-cds.org/2016.html\n",
    "\n",
    "https://www.trec-cds.org/2023.html (quedan dos meses, el contenido es más parecido a lo que está en las diferentes secciones de MIMIC, no tenemos nada de referencia... de 2022 no subieron los qrels)\n",
    "\n",
    "Judgment (\"qrels\") file. Judgment of 0 is non-relevant, 1 is excluded, and 2 is eligible.\n",
    "\n",
    "* 2014 y 2015 -- pmc-text-02.tar.gz 2014-01-2 (no coinciden los ids), si coinciden con lo que está en ``ir_datasets``.\n",
    "* 2021 y 2022 -- ClinicalTrials.2021-04-27.part1.zip (están todos los ids)\n",
    "* ahdoc - clinicaltrials.gov-16_dec_2015 (faltan 8 ids sobre el de 2016, si lo hago sobre el de 2021, solo faltan 2 ids)\n",
    "    * Si bien este está pensado para clinical trials, como los casos son los mismos que para 2014 y 2015, se podrían usar las keywords en la evalución de medical documents.\n",
    "\n",
    "* Save pickle for all queries.\n",
    "* Save pickle for all qrels.\n",
    "* Save pickles for clinical trials.\n",
    "    * One per folder + one summarizing everything.\n",
    "* Save pickle for documents\n",
    "    * https://github.com/titipata/pubmed_parser --> Un parser que funciona bien! Permite obtener metadatos + texto\n",
    "    \n",
    "NOTA: Los ids de los documentos de pubmed NO coinciden con los que están en los judgements, peeero, ``ir_datasets`` tiene el dataset ya procesado. Baja los mismos archivos que bajé de la página... deberían ser los mismos sets que ya tengo.\n",
    "Al bajarlo, luego crea un doc_store y en teoría queda todo listo para ser usado. (https://github.com/allenai/ir_datasets)\n",
    "    \n",
    "TODO: En los clinical trials podrían haber quedado algunos \\n dependiendo de dónde se hayan generado los archivos    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21a95bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43280831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# podría haber usado pandas :face-palm:\n",
    "def get_topics(file_name): # {file_name : {name: XX, topics: [{number: XX, type_:XX, description: XX, summary: XX}]}\n",
    "    with open(file_name, 'r',encoding='utf8') as f:\n",
    "        data = f.read()\n",
    "    data = BeautifulSoup(data, \"xml\")\n",
    "    topic_list = []\n",
    "    topics = data.find('topics') # puede dar vacío\n",
    "    if topics is not None:\n",
    "        task = topics.get('task')\n",
    "        for topic in topics.find_all('topic'):\n",
    "            number = topic.get('number')\n",
    "            summary = topic.summary\n",
    "            if summary is not None:\n",
    "                summary = BeautifulSoup(summary.text,'html').text.strip()\n",
    "                description = BeautifulSoup(topic.description.text,'html').text.strip()\n",
    "            else:\n",
    "                description = BeautifulSoup(topic.text,'html').text.strip()\n",
    "            type_ = topic.get('type')\n",
    "\n",
    "            tt = {'number':number, 'description':description}\n",
    "            if type_ is not None:\n",
    "                tt['type'] = type_\n",
    "            if summary is not None:\n",
    "                tt['summary'] = summary\n",
    "            topic_list.append(tt)\n",
    "        return topic_list\n",
    "    for topic in data.find_all('TOP'):\n",
    "        topic_list.append({'number':topic.NUM.text, 'description':BeautifulSoup(topic.TITLE.text,'html').text.strip()})\n",
    "    return topic_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140457bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_judgements(file_name): # trec judgements\n",
    "    df = pd.read_csv(file_name,sep=' ',header=None) \n",
    "    if len(df.columns) == 1:\n",
    "          df = pd.read_csv(file_name,sep='\\t',header=None) \n",
    "\n",
    "    df = df.drop(columns=[1]) # 1 se puede borrar\n",
    "\n",
    "    relevants = defaultdict(dict) # {topic : {document : score}}\n",
    "    relevant_score = defaultdict(defaultdict(set).copy) # {topic: {score : {documents}}}\n",
    "    for i in tqdm(range(0,len(df))):\n",
    "        relevants[df[0].values[i]][df[2].values[i]] = df[3].values[i]\n",
    "        relevant_score[df[0].values[i]][df[3].values[i]].add(df[2].values[i])\n",
    "        \n",
    "    return relevants, relevant_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e21bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_files = {} # matching = {name : {topics: XX, qrels: XX}} \n",
    "matching_files['topics2014.xml'] = 'qrels2014.txt'\n",
    "matching_files['topics2015B.xml'] = 'qrels-treceval-2015.txt'\n",
    "matching_files['topics2021.xml'] = 'qrels2021.txt'\n",
    "# matching_files['topics2022.xml'] = None\n",
    "matching_files['adhoc-queries.json'] = 'qrels-clinical_trials.txt'\n",
    "\n",
    "matching_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852ce69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_topics = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e05230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {}\n",
    "judgements = {}\n",
    "\n",
    "for t,j in matching_files.items():\n",
    "    if not t.endswith('.json'):\n",
    "        topics[t] = get_topics(dir_topics + t)  \n",
    "    judgements[j] = get_judgements(dir_topics + j)\n",
    "    \n",
    "with open(dir_topics + '__all_judgements.pickle','wb') as file:\n",
    "    pickle.dump(judgements,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f873c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(dir_topics + 'adhoc-queries.json','r') as file:\n",
    "    jj = json.load(file)\n",
    "\n",
    "for i in tqdm(range(0,len(jj))):\n",
    "    topic = jj[i]\n",
    "    topic['number'] = topic['qId']\n",
    "    del topic['qId']\n",
    "    topic['task'] = topic['queryType']\n",
    "    del topic['queryType']\n",
    "    \n",
    "    orig = topic['number'].split('-')\n",
    "    orig[0] = 'topics2014' if orig[0] == 'trec2014' else 'topics2015B' \n",
    "    for tt in topics[orig[0]+'.xml']:\n",
    "        if tt['number'] == orig[1]:\n",
    "            topic['summary'] = tt['summary']\n",
    "            break\n",
    "\n",
    "topics['adhoc-queries.json'] = jj\n",
    "with open(dir_topics + '__all_topics.pickle','wb') as file:\n",
    "    pickle.dump(topics,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddf7783",
   "metadata": {},
   "source": [
    "----------------------------------\n",
    "\n",
    "Clinical trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6b441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict\n",
    "\n",
    "import re\n",
    "re_spaces = re.compile('\\n\\s+')\n",
    "\n",
    "def load_clinical_trial(file_name=None,text=None):\n",
    "    \n",
    "    def process_textblock(text):\n",
    "        xx = [] \n",
    "        text = text.replace('\\r','')\n",
    "        for q in text.split('\\n\\n'): \n",
    "            x = q.split('\\n') \n",
    "            x = [z.strip() for z in x] # hay que mergear lo que haya quedado mal del tokenizer\n",
    "            for i in range(1,len(x)):\n",
    "                if x[i] == '':\n",
    "                    continue\n",
    "                j = i-1\n",
    "                while x[j] == '':\n",
    "                    j = j-1\n",
    "                if not x[i][0].isupper() and not x[j][-1] == '.':\n",
    "                    x[j] = x[j] + ' ' + x[i]\n",
    "                    x[i] = ''\n",
    "            x = [z.strip() for z in x if len(z) > 0] \n",
    "            xx.extend(x)\n",
    "        aa = []\n",
    "        title = None\n",
    "        crit = {}\n",
    "        for z in xx:\n",
    "            z = z.strip()\n",
    "            if z.endswith(':'):\n",
    "                if title is not None:\n",
    "                    crit[title] = aa\n",
    "                title = z[:-1].lower()\n",
    "                aa = []\n",
    "            elif title is not None:\n",
    "                aa.append(z.strip() if not z.startswith('-') else z[1:].strip())\n",
    "        if title is not None and len(aa) > 0:\n",
    "            crit[title] = aa\n",
    "        if len(crit) == 0:\n",
    "            for l in xx:\n",
    "                l = l.split(':')\n",
    "                if l[0].isupper():\n",
    "                    crit[l[0]] = ' '.join(l[1:]).strip()\n",
    "        if len(crit) == 0:\n",
    "            crit = xx\n",
    "        return crit           \n",
    "    \n",
    "    if file_name is not None:\n",
    "        with open(file_name,encoding='utf8') as xml_file:\n",
    "            d = xmltodict.parse(xml_file.read())\n",
    "    else:\n",
    "        d = xmltodict.parse(text)\n",
    "    \n",
    "    if 'eligibility' in d['clinical_study'] and 'criteria' in d['clinical_study']['eligibility'] and 'textblock' in d['clinical_study']['eligibility']['criteria']:\n",
    "        d['clinical_study']['eligibility']['criteria']['textblock'] = process_textblock(d['clinical_study']['eligibility']['criteria']['textblock'])\n",
    "    \n",
    "    if 'eligibility' in d['clinical_study'] and 'study_pop' in d['clinical_study']['eligibility'] and 'textblock' in d['clinical_study']['eligibility']['study_pop']:\n",
    "        d['clinical_study']['eligibility']['study_pop']['textblock'] = process_textblock(d['clinical_study']['eligibility']['study_pop']['textblock'])\n",
    "    \n",
    "    if 'brief_summary' in d['clinical_study'] and 'textblock' in d['clinical_study']['brief_summary']:\n",
    "        d['clinical_study']['brief_summary']['textblock'] = process_textblock(d['clinical_study']['brief_summary']['textblock'])\n",
    "    \n",
    "    if 'detailed_description' in d['clinical_study'] and 'textblock' in d['clinical_study']['detailed_description']:\n",
    "        d['clinical_study']['detailed_description']['textblock'] = process_textblock(d['clinical_study']['detailed_description']['textblock'])\n",
    "    \n",
    "    d = d['clinical_study']\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "567e9c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_trials_dir = 'C:/Users/Anto/Desktop/clinical_trials/'\n",
    "clinical_trials_dir = 'D:/clinical_trials/'\n",
    "prefix = 'ClinicalTrials.2021-04-27'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39f8701",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_trials = {}\n",
    "for dd in tqdm(os.listdir(clinical_trials_dir + prefix + '/')):\n",
    "    \n",
    "    if os.path.exists(clinical_trials_dir + prefix + '__'+ dd + '.pickle'):\n",
    "        continue\n",
    "    \n",
    "    for ct in tqdm(os.listdir(clinical_trials_dir + dd)):\n",
    "        ct_file = clinical_trials_dir + dd + '/' + ct\n",
    "        print(ct_file)\n",
    "        ct_dict = load_clinical_trial(ct_file)\n",
    "        clinical_trials[ct[0:-4]] = ct_dict\n",
    "        \n",
    "    with open(clinical_trials_dir + prefix + '__'+ dd + '.pickle','wb') as file:\n",
    "        pickle.dump(clinical_trials,file)\n",
    "    clinical_trials.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca3085c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# de los pickles, seleccionar solo el summary, description y eligibility para reducir un poco\n",
    "from collections import defaultdict\n",
    "\n",
    "def summarize_trials(dir_,prefix):\n",
    "    reduced_trials = {}\n",
    "    for ff in tqdm(os.listdir(dir_)):\n",
    "        if not ff.endswith('.pickle') or not ff.startswith(prefix):\n",
    "            continue\n",
    "        trials = pd.read_pickle(dir_ + ff)\n",
    "\n",
    "        for k,d in tqdm(trials.items()):\n",
    "            tt = defaultdict(defaultdict(dict).copy)\n",
    "            if 'eligibility' in d and 'criteria' in d['eligibility'] and 'textblock' in d['eligibility']['criteria']:\n",
    "                tt['eligibility']['criteria']['textblock'] = d['eligibility']['criteria']['textblock'] \n",
    "\n",
    "            if 'brief_summary' in d and 'textblock' in d['brief_summary']:\n",
    "                tt['brief_summary']['textblock'] = d['brief_summary']['textblock']\n",
    "\n",
    "            if 'detailed_description' in d and 'textblock' in d['detailed_description']:\n",
    "                tt['detailed_description']['textblock'] = d['detailed_description']['textblock'] \n",
    "            reduced_trials[k] = tt\n",
    "    return reduced_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b174640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduced_trials = summarize_trials(clinical_trials_dir,prefix)\n",
    "\n",
    "with open(clinical_trials_dir + prefix + '__document_summary.pickle','wb') as file:\n",
    "    pickle.dump(reduced_trials,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf80078",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'clinicaltrials.gov-16_dec_2015'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91f3357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c908a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = tarfile.open(clinical_trials_dir + prefix + \".tgz\", \"r:gz\")\n",
    "\n",
    "clinical_trials_2015 = {}\n",
    "\n",
    "if os.path.exists(clinical_trials_dir + prefix + '__document_summary.pickle'):\n",
    "    processed = pd.read_pickle(clinical_trials_dir + prefix + '__document_summary.pickle').keys()\n",
    "else:\n",
    "    processed = set()\n",
    "    for ff in os.listdir(clinical_trials_dir):\n",
    "        if not ff.startswith(prefix) or not ff.endswith('.pickle'):\n",
    "            continue\n",
    "        processed.update(pd.read_pickle(clinical_trials_dir + ff).keys())\n",
    "        \n",
    "for member in tqdm(tar): # get_members is not a generator\n",
    "    f = tar.extractfile(member)\n",
    "    if f is None or member.name[member.name.rindex('/')+1:].startswith('._'):\n",
    "        continue\n",
    "        \n",
    "    if member.name[member.name.rindex('/')+1:] in processed:\n",
    "        continue\n",
    "        \n",
    "    clinical_trials_2015[member.name[member.name.rindex('/')+1:]] = load_clinical_trial(text=f.read())\n",
    "    \n",
    "    if len(clinical_trials_2015) % 4000 == 0:\n",
    "        with open(clinical_trials_dir + prefix + '_'+member.name[member.name.rindex('/')+1:]+'.pickle','wb') as file:\n",
    "            pickle.dump(clinical_trials_2015,file)\n",
    "        clinical_trials_2015.clear()\n",
    "\n",
    "if len(clinical_trials_2015) > 0:\n",
    "        with open(clinical_trials_dir + prefix + '_'+member.name[member.name.rindex('/')+1:]+'.pickle','wb') as file:\n",
    "            pickle.dump(clinical_trials_2015,file)\n",
    "        clinical_trials_2015.clear()\n",
    "        \n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef92f845",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduced_trials = summarize_trials(clinical_trials_dir,prefix)\n",
    "\n",
    "if os.path.exists(clinical_trials_dir + prefix + '__document_summary.pickle'):\n",
    "    reduced_trials.update(pd.read_pickle(clinical_trials_dir + prefix + '__document_summary.pickle'))\n",
    "\n",
    "with open(clinical_trials_dir + prefix + '__document_summary.pickle','wb') as file:\n",
    "    pickle.dump(reduced_trials,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74e7b0c",
   "metadata": {},
   "source": [
    "----------------------------------------\n",
    "\n",
    "Pubmed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad85ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_dir = 'C:/Users/Anto/Downloads/'\n",
    "doc_med_files = ['pmc-00.tar.gz', 'pmc-01.tar.gz', 'pmc-02.tar.gz', 'pmc-03.tar.gz'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7e9dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pubmed_parser as pp\n",
    "\n",
    "processed = set()\n",
    "for ff in tqdm(os.listdir(documents_dir)):\n",
    "    if not ff.startswith('__documents_'):\n",
    "        continue\n",
    "    processed.update(pd.read_pickle(documents_dir + ff).keys())\n",
    "\n",
    "print(len(processed))\n",
    "    \n",
    "docs = {}\n",
    "for i in range(0,len(doc_med_files)):\n",
    "    tar = tarfile.open(documents_dir + doc_med_files[i], \"r:gz\")\n",
    "    for member in tqdm(tar): \n",
    "        if not member.name.endswith('xml'):\n",
    "            continue\n",
    "            \n",
    "        if member.name[member.name.rindex('/')+1:-5] in processed:\n",
    "            continue\n",
    "            \n",
    "        tar.extract(member,documents_dir)\n",
    "#         print(documents_dir + member.name)\n",
    "        dict_out = pp.parse_pubmed_xml(documents_dir + member.name) # metadatos\n",
    "        dict_out['text'] = pp.parse_pubmed_paragraph(documents_dir + member.name,all_paragraph=True) # texto\n",
    "        os.remove(documents_dir + member.name) \n",
    "\n",
    "        docs[member.name[member.name.rindex('/')+1:-5]] = dict_out\n",
    "        if len(docs) % 1000 == 0:\n",
    "            with open(documents_dir + '__documents_'+member.name[member.name.rindex('/')+1:-5]+'.pickle','wb') as file:\n",
    "                pickle.dump(docs,file)\n",
    "            docs.clear()\n",
    "\n",
    "if len(docs) > 0:\n",
    "    with open(documents_dir + '__documents_'+member.name[member.name.rindex('/')+1:-5]+'.pickle','wb') as file:\n",
    "        pickle.dump(docs,file)\n",
    "    docs.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d0e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize documents in one pickle title, keywords (?), abstract\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def summarize_documents(dir_,prefix):\n",
    "    reduced_trials = {}\n",
    "    for ff in tqdm(os.listdir(dir_)):\n",
    "        if not ff.endswith('.pickle') or not ff.startswith(prefix):\n",
    "            continue\n",
    "        trials = pd.read_pickle(dir_ + ff)\n",
    "\n",
    "        for k,d in tqdm(trials.items()): # 'full_title', 'abstract', 'journal', 'pmid', 'pmc',\n",
    "            tt = defaultdict(defaultdict(dict).copy)\n",
    "            if 'full_title' in d:\n",
    "                tt['full_title'] = d['full_title']\n",
    "\n",
    "            if 'abstract' in d:\n",
    "                tt['abstract'] = d['abstract']\n",
    "\n",
    "            if 'journal' in d:\n",
    "                tt['journal'] = d['journal']\n",
    "                \n",
    "            if 'pmid' in d:\n",
    "                tt['pmid'] = d['pmid']\n",
    "                \n",
    "            if 'pmc' in d:\n",
    "                tt['pmc'] = d['pmc']\n",
    "                \n",
    "            reduced_trials[k] = tt\n",
    "    return reduced_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629aea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents_dir = 'D:/'\n",
    "prefix = '__documents_'\n",
    "\n",
    "reduced_trials = summarize_documents(documents_dir,prefix)\n",
    "\n",
    "if os.path.exists(documents_dir + prefix + '__document_summary.pickle'):\n",
    "    reduced_trials.update(pd.read_pickle(documents_dir + prefix + '__document_summary.pickle'))\n",
    "\n",
    "with open(documents_dir + prefix + '__document_summary.pickle','wb') as file:\n",
    "    pickle.dump(reduced_trials,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56fcfc2",
   "metadata": {},
   "source": [
    "Chequear que no faltan documentos de los que necesitamos, tanto pmc como clinical trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9eb3a8f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['qrels2014.txt', 'qrels-treceval-2015.txt', 'qrels2021.txt', 'qrels-clinical_trials.txt'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_judgements = pd.read_pickle(clinical_trials_dir + '__all_judgements.pickle')\n",
    "all_judgements.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "462e8923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "939770"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = pd.read_pickle(clinical_trials_dir + '__documents___document_summary.pickle')\n",
    "# documents = pd.read_pickle(clinical_trials_dir + 'ClinicalTrials.2021-04-27__document_summary.pickle')\n",
    "# documents = pd.read_pickle(clinical_trials_dir + 'clinicaltrials.gov-16_dec_2015__document_summary.pickle')\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46c74bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3271"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = set()\n",
    "for jj in all_judgements['qrels2014.txt'][1].values():# por cada topic, {score : {docs}}\n",
    "    docs.update(jj[1])\n",
    "    docs.update(jj[2])\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08dca792",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs - set(documents.keys()) # en 2016 requiere sacar xml del nombre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519c3401",
   "metadata": {},
   "source": [
    "En teoría, los datasets de documentos de pmc están disponibles en ``ir_datasets``, y se pueden pasar casi directamente a ``pyterrier`` para ser indexados.\n",
    "\n",
    "Baja los datasets y los descomprime, no incluyen el body, solo el abstract.\n",
    "En el dataset bajado hay 147k documentos que no están en el otro... Raro. \n",
    "Lo más raro, es que los ids de los documentos relevantes no coinciden con los del trec!\n",
    "\n",
    "Como solución \"simple\", guardar todo esto nuevo y usarlo desde las estructuras que ya había creado.\n",
    "\n",
    "* La de 2015 usa los mismos documentos, con lo que solo hay que guardar extra los qrels y los topics.\n",
    "* Los tópicos son iguales en ambos casos.\n",
    "* Los qrels son los mismos en ambos casos.\n",
    "\n",
    "https://ir-datasets.com/pmc.html#pmc/v1/trec-cds-2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebeaa5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "dataset = ir_datasets.load(\"pmc/v1/trec-cds-2014\") # son iguales que los de 2015!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4f8efc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b283793b1af4159abf7a60bc7928f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "re_spaces = re.compile('\\s+')\n",
    "docs_ir = {} \n",
    "\n",
    "i = 0\n",
    "\n",
    "for query in tqdm(dataset.docs_iter()):\n",
    "    docs_ir[query[0]] = {'journal' : query[1], 'title':query[2], \n",
    "                         'abstract':query[3],\n",
    "                         'body': re_spaces.sub(' ',query[4].replace('\\n',''))}  # namedtuple<doc_id, journal, title, abstract, body>\n",
    "    i += 1\n",
    "    if i % 100000 == 0:\n",
    "        with open(clinical_trials_dir + '__documents_ir_2014_body_'+str(i)+'.pickle','wb') as file:\n",
    "            pickle.dump(docs_ir,file)\n",
    "        docs_ir.clear()\n",
    "\n",
    "if len(docs_ir) > 0:\n",
    "    with open(clinical_trials_dir + '__documents_ir_2014_body_'+str(i)+'.pickle','wb') as file:\n",
    "            pickle.dump(docs_ir,file)\n",
    "    docs_ir.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af8ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "qrels = defaultdict(dict) # {topic : {document : score}}\n",
    "qrels_scores = defaultdict(defaultdict(set).copy) # {topic: {score : {documents}}}\n",
    "\n",
    "for query in tqdm(dataset.qrels_iter()):\n",
    "    qrels[int(query[0])][int(query[1])] = int(query[2])\n",
    "    qrels_scores[int(query[0])][int(query[2])].add(int(query[1]))\n",
    "\n",
    "with open(clinical_trials_dir + 'qrles_ir_2014.pickle','wb') as file:\n",
    "    pickle.dump((qrels,qrels_scores),file)    \n",
    "len(qrels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e1384800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd42c06c4e047b9809ac951e455ca94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "queries = []\n",
    "for query in tqdm(dataset.queries_iter()):\n",
    "    queries.append({'number': query[0], 'type':query[1],'description':query[2],'summary':query[3]})\n",
    "    \n",
    "with open(clinical_trials_dir + 'queries_ir_2014.pickle','wb') as file:\n",
    "    pickle.dump(queries,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "360529ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "dataset = ir_datasets.load(\"pmc/v1/trec-cds-2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a3718027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f349dd445d8f4a3eabcb83a106ac819b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels = defaultdict(dict) # {topic : {document : score}}\n",
    "qrels_scores = defaultdict(defaultdict(set).copy) # {topic: {score : {documents}}}\n",
    "\n",
    "for query in tqdm(dataset.qrels_iter()):\n",
    "    qrels[int(query[0])][int(query[1])] = int(query[2])\n",
    "    qrels_scores[int(query[0])][int(query[2])].add(int(query[1]))\n",
    "\n",
    "with open(clinical_trials_dir + 'qrles_ir_2015.pickle','wb') as file:\n",
    "    pickle.dump((qrels,qrels_scores),file)    \n",
    "len(qrels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "03b64750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89f39f8db05430f9b9be3ce73b793ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "queries = []\n",
    "for query in tqdm(dataset.queries_iter()):\n",
    "    queries.append({'number':str(query[0]), 'type':query[1],'description':query[2],'summary':query[3]})\n",
    "    \n",
    "with open(clinical_trials_dir + 'queries_ir_2015.pickle','wb') as file:\n",
    "    pickle.dump(queries,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0f5970bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['qrels2014.txt', 'qrels-treceval-2015.txt', 'qrels2021.txt', 'qrels-clinical_trials.txt'])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_judgements = pd.read_pickle(clinical_trials_dir + '__all_judgements.pickle')\n",
    "all_judgements.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
